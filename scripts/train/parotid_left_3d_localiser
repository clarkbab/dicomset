#! /usr/bin/env python
import argparse
from argparse import ArgumentParser, ArgumentTypeError
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from torchio.transforms import Compose, CropOrPad, RandomAffine, RandomElasticDeformation, RandomNoise, Resample
from typing import *

root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(root_dir)

from mymi.loaders import ParotidLeft3DLocaliserLoader, ParotidLeft3DLocaliserVisualLoader
from mymi.losses import DiceLoss
from mymi.models import SingleChannelUNet
from mymi.training import ModelTrainer
from mymi import transforms as ts
from mymi import utils

# Parse options.
def interval_type(s: str):
    if s == 'epoch':
        return s
    elif s.isdigit():
        return int(s)
    else:
        raise ArgumentTypeError(f"Should be 'epoch' or integer, got '{s}'.")

parser = ArgumentParser(description="""
Train the segmentation model on a single node. Spins up a separate process for each GPU on the node (--num-gpus).
""")
parser.add_argument('--batch-size', action='store', type=int, default=1, help='sets the batch size, default=1')
parser.add_argument('--data-dir', action='store', help='sets the data directory')
parser.add_argument('--learning-rate', action='store', type=float, help="sets the learning rate")
parser.add_argument('--log-level', action='store', default='info', help="sets the log level, e.g. 'debug', 'info'")
parser.add_argument('--loss', action='store', default='dice', help="sets the loss function, e.g. 'cross-entropy' or 'dice', default='dice'")
parser.add_argument('--max-epochs', action='store', type=int, help='sets the maximum number of epochs')
parser.add_argument('--name', action='store', help='sets the run name for Tensorboard')
parser.add_argument('--no-gpu', action='store_true', default=False, help='use CPU for training')
parser.add_argument('--no-mixed', action='store_true', default=False, help="don't use mixed precision for training")
parser.add_argument('--no-report', action='store_true', default=False, help="don't send run reports")
parser.add_argument('--no-validate', action='store_true', default=False, help="don't validate the model during training")
parser.add_argument('--node-rank', action='store', type=int, default=0, help="the node's rank, default=0")
parser.add_argument('--num-gpus', action='store', type=int, default=1, help='sets the number of GPUs per node, default=0')
parser.add_argument('--num-nodes', action='store', type=int, default=1, help='sets the number of nodes, default=1')
parser.add_argument('--train-print-interval', action='store', type=interval_type, help='iterations between printing during training')
parser.add_argument('--train-report-interval', action='store', type=interval_type, help='iterations between recording during training')
parser.add_argument('--use-aug', action='store_true', default=False, help='use data augmentations when training')
parser.add_argument('--use-dropout', action='store_true', default=False, help='use dropout when training')
parser.add_argument('--use-early-stopping', action='store_true', default=False, help='use early stopping')
parser.add_argument('--use-elastic-aug', action='store_true', default=False, help='use elastic deformation augmentation')
parser.add_argument('--validate-interval', action='store', type=interval_type, help='iterations between validation')
parser.add_argument('--validate-print-interval', action='store', type=interval_type, help='iterations between printing during validation')
parser.add_argument('--validate-report-interval', action='store', type=interval_type, help='iterations between reporting during validation')
args = parser.parse_args()

# Returns a logging function for the process.
def get_log_info(args: dict):
    def log_info(s: str):
        if is_multi_process(args):
            if is_lead_process(args):
                s = f"[Leader - {os.getpid()}] {s}"
            else:
                s = f"[{os.getpid()}] {s}" 
        logging.info(s)
    return log_info

# Define single GPU/process training method.
def train(args: dict):
    # Get log function for process.
    log_info = get_log_info(args)

    if is_multi_process(args):
        # Register the process. This action blocks until all processes in 'world_size' have registered.
        rank = args.node_rank * args.num_gpus + args.gpu_idx
        log_info(f"Initialising process {args.gpu_idx}, with rank {rank} and world size {args.world_size}.")
        dist.init_process_group(backend='nccl', init_method='file:///tmp/dist', world_size=args.world_size, rank=rank)
    
    # Determine model and run name.
    model_name = 'parotid-left-3d-localiser'
    if args.name:
        run_name = args.name
    else:
        run_name = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
    log_info(f"Model: {model_name}, Run: {run_name}.")

    # Configure device.
    if args.no_gpu:
        device = torch.device('cpu')
    else:
        if torch.cuda.is_available():
            if args.num_gpus > 1:
                device = torch.device(f"cuda:{args.gpu_idx}")
            else:
                device = torch.device('cuda:0')
        else:
            device = torch.device('cpu')
            log_info('CUDA not available.')
    log_info(f"Using device: {device}.")

    # Create model.
    model = SingleChannelUNet(dropout=args.use_dropout)
    model = model.to(device)
    log_info(f"Using model: {model.__class__}, Dropout: {args.use_dropout}.")

    # Wrap model for distributed training.
    if is_multi_process(args):
        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu_idx])

    # Define loss function.
    if args.loss == 'bce':
        class_weights = torch.Tensor((1, 6767.77)).to(device)      # Calculated from 'dataset.class_frequencies'.
        loss_fn = nn.CrossEntropyLoss(weight=class_weights)
    elif args.loss == 'dice':
        loss_fn = DiceLoss()
    else:
        raise ValueError(f"Loss '{args.loss}' not recognised.")
    log_info(f"Using loss function: {loss_fn.__class__}.")

    # Define optimiser.
    if args.learning_rate is not None:
        lr = args.learning_rate
    else:
        lr = 0.001
    optimiser = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    log_info(f"Using optimiser: {optimiser}.")

    # Define required transforms.
    new_spacing = (4, 4, 6.625)         # Largest volume with size (512, 512, 212) @ spacing (1, 1, 3) will be resampled to (128, 128, 96).
    size = (128, 128, 96)               # Network's expected input size.
    required_pre_transforms = [
        Resample(new_spacing)
    ]
    required_post_transforms = [
        CropOrPad(size)
    ]

    # Define augmentations.
    rotation = (-5, 5)
    translation = (-50, 50)
    scale = (0.8, 1.2)
    aug_transforms = [
        RandomAffine(degrees=rotation, scales=scale, translation=translation, default_pad_value='minimum')
    ]
    if args.use_elastic_aug:
        aug_transforms += [
            RandomElasticDeformation()
        ]

    # Compose transforms.
    if args.use_aug:
        train_transforms = required_pre_transforms + aug_transforms + required_post_transforms
    else:
        train_transforms = required_pre_transforms + required_post_transforms
    log_info(f"Using transforms '{train_transforms}'.")
    train_transform = Compose(train_transforms)
    validate_transform = Compose(required_pre_transforms + required_post_transforms)

    # Create data loaders.
    batch_size = args.batch_size
    spacing = (1, 1, 3)         # Data on disk has this spacing.
    train_loader = ParotidLeft3DLocaliserLoader.build('train', batch_size=batch_size, spacing=spacing, transform=train_transform)
    validate_loader = ParotidLeft3DLocaliserLoader.build('validate', batch_size=batch_size, spacing=spacing, transform=validate_transform)
    visual_loader = ParotidLeft3DLocaliserVisualLoader.build(batch_size=batch_size, spacing=spacing, transform=validate_transform)

    # Create model trainer kwargs.
    kwargs = {}
    kwargs['device'] = device
    kwargs['log_info'] = log_info
    if args.max_epochs:
        kwargs['max_epochs'] = args.max_epochs
    if args.no_mixed:
        kwargs['mixed_precision'] = False
        log_info('Using full precision.')
    else:
        log_info('Using mixed precision.')
    if args.no_report:
        kwargs['report'] = False
        log_info('Not reporting results.')
    else:
        if is_lead_process(args):
            kwargs['report'] = True
            log_info('Reporting results.')
        else:
            kwargs['report'] = False
    if args.no_validate:
        kwargs['validate'] = False
        log_info('Not validating.')
    else:
        if is_lead_process(args):
            kwargs['validate'] = True
            log_info('Validating.')
        else:
            kwargs['validate'] = False
    if args.train_print_interval:
        kwargs['train_print_interval'] = args.train_print_interval
    if args.train_report_interval:
        kwargs['train_report_interval'] = args.train_report_interval
    kwargs['spacing'] = (1., 1., 3.)
    if args.use_early_stopping:
        kwargs['early_stopping'] = True
        log_info('Using early stopping.')
    else:
        log_info('Not using early stopping.')
    if args.validate_interval:
        kwargs['validate_interval'] = args.validate_interval
    if args.validate_print_interval:
        kwargs['validate_print_interval'] = args.validate_print_interval
    if args.validate_report_interval:
        kwargs['validate_report_interval'] = args.validate_report_interval

    # Train the model.
    trainer = ModelTrainer(loss_fn, model_name, optimiser, run_name, train_loader, validate_loader, visual_loader, **kwargs)
    trainer(model)

def is_multi_process(args: dict):
    return (not args.no_gpu) and args.num_gpus > 1

def is_lead_process(args: dict):
    return not is_multi_process(args) or args.gpu_idx == 0

# Wrap 'train' function to fit multi-processing API.
def train_process(gpu_idx: int, args: dict):
    args.gpu_idx = gpu_idx
    train(args)

def prepend(s: str):
    if __name__ == '__main__':
        return f"[Main - {os.getpid()}] {s}"
    else:
        return f"[{os.getpid()}] {s}"

# Configure logging.
log_level = getattr(logging, args.log_level.upper(), None)
utils.configure_logging(log_level)

if __name__ == '__main__':
    if not args.no_gpu:
        # Print GPU info.
        num_devices = torch.cuda.device_count()
        logging.info(f"[Main] Requested {args.num_gpus} GPUs, found {num_devices} GPUs.")
        assert args.num_gpus <= num_devices

    if is_multi_process(args):
        # Spawn a process for each requested GPU.
        logging.info(f"[Main] Spawning {args.num_gpus} process/es.")
        args.world_size = args.num_nodes * args.num_gpus
        mp.spawn(train_process, nprocs=args.num_gpus, args=(args, ))
    else:
        train(args)
