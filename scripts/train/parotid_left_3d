#! /usr/bin/env python
import argparse
from argparse import ArgumentParser, ArgumentTypeError
import logging
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from torchio.transforms import Compose, CropOrPad, RandomAffine, RandomElasticDeformation, RandomNoise, Resample
from typing import *

root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(root_dir)

from mymi.loaders import ParotidLeft3DLoader, ParotidLeft3DVisualLoader
from mymi.losses import DiceLoss
from mymi.models import ParotidLeft3DNet
from mymi.training import ModelTrainer
from mymi import transforms as ts
from mymi import utils

# Parse options.
def interval_type(s: str):
    if s == 'epoch':
        return s
    elif s.isdigit():
        return int(s)
    else:
        raise ArgumentTypeError(f"Should be 'epoch' or integer, got '{s}'.")

parser = ArgumentParser(description="""
Train the segmentation model on a single node. Spins up a separate process for each GPU on the node (--num-gpus).
""")
parser.add_argument('-a', '--use-aug', action='store_true', default=False, help='use data augmentations when training')
parser.add_argument('-b', '--batch-size', action='store', type=int, default=1, help='sets the batch size, default=1')
parser.add_argument('-c', '--checkpoint-dir', action='store', help='sets the checkpoint directory')
parser.add_argument('-d', '--data-dir', action='store', help='sets the data directory')
parser.add_argument('-e', '--max-epochs', action='store', type=int, help='sets the maximum number of epochs')
parser.add_argument('-g', '--num-gpus', action='store', type=int, default=1, help='sets the number of GPUs per node, default=0')
parser.add_argument('-l', '--log-level', action='store', default='info', help="sets the log level, e.g. 'debug', 'info'")
parser.add_argument('-ls', '--loss', action='store', default='dice', help="sets the loss function, e.g. 'cross-entropy' or 'dice', default='dice'")
parser.add_argument('-m', '--use-mixed', action='store_true', default=False, help='use mixed precision for training')
parser.add_argument('-n', '--name', action='store', help='sets the run name for Tensorboard')
parser.add_argument('-no', '--num-nodes', action='store', type=int, default=1, help='sets the number of nodes, default=1')
parser.add_argument('-nr', '--node-rank', action='store', type=int, default=0, help="the node's rank, default=0")
parser.add_argument('-p', '--print-interval', action='store', type=interval_type, help='iterations between printing')
parser.add_argument('-r', '--record-interval', action='store', type=interval_type, help='iterations between recording')
parser.add_argument('-t', '--tensorboard-dir', action='store', help='sets the tensorboard directory')
parser.add_argument('-u', '--use-gpu', action='store_true', default=False, help='use GPU for training')
parser.add_argument('-v', '--validation-interval', action='store', type=interval_type, help='iterations between validation')
args = parser.parse_args()

# Returns a logging function for the process.
def get_log_info(args: dict):
    def log_info(s: str):
        if is_multi_process(args):
            if is_lead_process(args):
                s = f"[Leader - {os.getpid()}] {s}"
            else:
                s = f"[{os.getpid()}] {s}" 
        logging.info(s)
    return log_info

# Define single GPU/process training method.
def train(args: dict):
    # Get log function for process.
    log_info = get_log_info(args)

    if is_multi_process(args):
        # Register the process. This action blocks until all processes in 'world_size' have registered.
        rank = args.node_rank * args.num_gpus + args.gpu_idx
        log_info(f"Initialising process {args.gpu_idx}, with rank {rank} and world size {args.world_size}.")
        dist.init_process_group(backend='nccl', init_method='file:///tmp/dist', world_size=args.world_size, rank=rank)

    # Define transforms.
    fill = 'minimum'
    rotation = (-5, 5)
    translation = (-50, 50)
    scale = (0.8, 1.2)
    lam = 1.0
    high_res = (512, 512, 212)
    low_res = (128, 128, 96)
    spacing = (1, 1, 3)
    low_res_spacing = tuple((np.array(high_res) / low_res) * spacing)
    aug_transforms = [                                               # Only applied when augmenting.
        RandomAffine(degrees=rotation, scales=scale, translation=translation, default_pad_value=fill)
    ]
    required_transforms = [                                             # Applied to all images to satisfy network architecture constraints.
        CropOrPad(high_res, padding_mode=fill),
        Resample(low_res_spacing)
    ]
    if args.use_aug:
        train_transforms = aug_transforms + required_transforms
    else:
        train_transforms = required_transforms

    logging.info(f"Using transforms '{train_transforms}'.")
    train_transform = Compose(train_transforms)
    validation_transform = Compose(required_transforms)

    # Get data loaders.
    batch_size = args.batch_size
    opt_kwargs = {}
    if args.data_dir is not None: opt_kwargs['data_dir'] = args.data_dir
    train_loader = ParotidLeft3DLoader.build('train', batch_size=batch_size, transform=train_transform, **opt_kwargs)
    validation_loader = ParotidLeft3DLoader.build('validate', batch_size=batch_size, transform=validation_transform, **opt_kwargs)
    visual_loader = ParotidLeft3DVisualLoader.build(batch_size=batch_size, transform=validation_transform, **opt_kwargs)

    # Configure device.
    if args.use_gpu:
        assert torch.cuda.is_available()
        if args.num_gpus > 1:
            device = torch.device(f"cuda:{args.gpu_idx}")
            log_info(f"Using GPU ({device}).")
        else:
            device = torch.device('cuda:0')
            log_info(f"Using GPU ({device}).")
    else:
        log_info('Using CPU.')
        device = torch.device('cpu')

    # Create model.
    model = ParotidLeft3DNet()
    model = model.to(device)

    # Wrap for distributed training.
    if is_multi_process(args):
        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu_idx])

    # Define loss.
    if args.loss == 'cross-entropy':
        class_weights = torch.Tensor((1, 6767.77)).to(device)      # Calculated from 'dataset.class_frequencies'.
        loss_fn = nn.CrossEntropyLoss(weight=class_weights)
        log_info(f"Using cross-entropy loss.")
    elif args.loss == 'dice':
        loss_fn = DiceLoss()
        log_info(f"Using dice loss.")

    # Define optimiser.
    optimiser = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    log_info(f"Using optimiser: {optimiser}.")

    # Create model trainer keyword arguments.
    opt_kwargs = {}
    if args.checkpoint_dir is not None: opt_kwargs['checkpoint_dir'] = args.checkpoint_dir
    if is_reporter(args): opt_kwargs['is_reporter'] = True
    if args.max_epochs is not None: opt_kwargs['max_epochs'] = args.max_epochs
    opt_kwargs['mixed_precision'] = args.use_mixed
    if args.use_mixed:
        log_info('Using mixed precision.')
    if args.name is not None: opt_kwargs['run_name'] = args.name
    if args.print_interval is not None: opt_kwargs['print_interval'] = args.print_interval
    if args.record_interval is not None: opt_kwargs['record_interval'] = args.record_interval
    if args.validation_interval is not None: opt_kwargs['validation_interval'] = args.validation_interval

    # Train the model.
    trainer = ModelTrainer(train_loader, validation_loader, optimiser, loss_fn, visual_loader, 
        device=device, log_info=log_info, **opt_kwargs)
    trainer(model)

def is_multi_process(args: dict):
    return args.use_gpu and args.num_gpus > 1

def is_lead_process(args: dict):
    return args.gpu_idx == 0

def is_reporter(args: dict):
    return not is_multi_process(args) or is_lead_process(args)

# Wrap 'train' function to fit multi-processing API.
def train_process(gpu_idx: int, args: dict):
    args.gpu_idx = gpu_idx
    train(args)

def prepend(s: str):
    if __name__ == '__main__':
        return f"[Main - {os.getpid()}] {s}"
    else:
        return f"[{os.getpid()}] {s}"

# Configure logging.
log_level = getattr(logging, args.log_level.upper(), None)
utils.configure_logging(log_level)

if __name__ == '__main__':
    if args.use_gpu:
        # Print GPU info.
        num_devices = torch.cuda.device_count()
        logging.info(f"[Main] Requested {args.num_gpus} GPUs, found {num_devices} GPUs.")
        assert args.num_gpus <= num_devices

    if is_multi_process(args):
        # Spawn a process for each requested GPU.
        logging.info(f"[Main] Spawning {args.num_gpus} process/es.")
        args.world_size = args.num_nodes * args.num_gpus
        mp.spawn(train_process, nprocs=args.num_gpus, args=(args, ))
    else:
        train(args)
